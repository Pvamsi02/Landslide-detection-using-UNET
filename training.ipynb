{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files labeled: 19799\n",
      "Number of files loaded: 19799\n",
      "Number of files labeled: 800\n",
      "Number of files loaded: 800\n"
     ]
    }
   ],
   "source": [
    "# train_cpu.ipynb\n",
    "\n",
    "# 1. Importing required libraries\n",
    "import os\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from model.Networks import unet\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from dataset.landslide_dataset import LandslideDataSet\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the eval_image function\n",
    "def eval_image(pred, label, num_classes):\n",
    "    TP = np.zeros((num_classes, 1))\n",
    "    FP = np.zeros((num_classes, 1))\n",
    "    TN = np.zeros((num_classes, 1))\n",
    "    FN = np.zeros((num_classes, 1))\n",
    "    n_valid_sample = 0\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        TP[i] = np.sum((pred == i) & (label == i))\n",
    "        FP[i] = np.sum((pred == i) & (label != i))\n",
    "        TN[i] = np.sum((pred != i) & (label != i))\n",
    "        FN[i] = np.sum((pred != i) & (label == i))\n",
    "        n_valid_sample += np.sum(label == i)\n",
    "\n",
    "    return TP, FP, TN, FN, n_valid_sample\n",
    "\n",
    "# 2. Defining the necessary configurations and settings\n",
    "data_dir = './dataset/'  # Directory where your dataset is located\n",
    "train_list = './dataset/train.txt'  # Path to the train.txt file\n",
    "test_list = './dataset/test.txt'  # Path to the test.txt file\n",
    "input_size = '128,128'  # Input size for the images\n",
    "num_classes = 2  # Number of classes (Landslide, Non-Landslide)\n",
    "batch_size = 32  # Batch size for training\n",
    "num_workers = 4  # Number of workers for data loading\n",
    "learning_rate = 1e-3  # Learning rate\n",
    "num_steps = 500  # Number of training steps\n",
    "num_steps_stop = 500  # Number of training steps for early stopping\n",
    "weight_decay = 5e-4  # Weight decay for regularization\n",
    "snapshot_dir = './snapshots/'  # Directory to save model snapshots\n",
    "\n",
    "# Ensure that the snapshot directory exists\n",
    "if not os.path.exists(snapshot_dir):\n",
    "    os.makedirs(snapshot_dir)\n",
    "\n",
    "# 3. Loading the data\n",
    "def load_data():\n",
    "    src_loader = data.DataLoader(\n",
    "        LandslideDataSet(data_dir, train_list, max_iters=num_steps_stop * batch_size, set='labeled'),\n",
    "        batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    test_loader = data.DataLoader(\n",
    "        LandslideDataSet(data_dir, test_list, set='unlabeled'),\n",
    "        batch_size=1, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return src_loader, test_loader\n",
    "\n",
    "src_loader, test_loader = load_data()\n",
    "\n",
    "# 4. Initializing the U-Net model\n",
    "model = unet(n_classes=num_classes)\n",
    "\n",
    "# 5. Defining the optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "cross_entropy_loss = nn.CrossEntropyLoss(ignore_index=255)\n",
    "\n",
    "# 6. Training function\n",
    "def train_model(num_steps_stop):\n",
    "    hist = np.zeros((num_steps_stop, 3))\n",
    "    for batch_id, src_data in enumerate(src_loader):\n",
    "        if batch_id == num_steps_stop:\n",
    "            break\n",
    "\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images, labels, _, _ = src_data\n",
    "\n",
    "        # Forward pass\n",
    "        pred = model(images)\n",
    "\n",
    "        # Compute loss\n",
    "        labels = labels.long()\n",
    "        loss = cross_entropy_loss(pred, labels)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate batch accuracy\n",
    "        _, predicted_labels = torch.max(pred, 1)\n",
    "        predicted_labels = predicted_labels.detach().cpu().numpy()\n",
    "        labels = labels.numpy()\n",
    "\n",
    "        batch_oa = np.sum(predicted_labels == labels) * 1.0 / len(labels.reshape(-1))\n",
    "        hist[batch_id, 0] = loss.item()\n",
    "        hist[batch_id, 1] = batch_oa\n",
    "        hist[batch_id, -1] = time.time() - start_time\n",
    "\n",
    "        # Print progress\n",
    "        if (batch_id + 1) % 10 == 0:\n",
    "            print(f'Iter {batch_id+1}/{num_steps} Time: {np.mean(hist[batch_id-9:batch_id+1,-1]):.2f} Batch_OA = {np.mean(hist[batch_id-9:batch_id+1,1])*100:.1f} cross_entropy_loss = {np.mean(hist[batch_id-9:batch_id+1,0]):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. Testing function\n",
    "def test_model():\n",
    "    print('Testing..........')\n",
    "    model.eval()\n",
    "    TP_all = np.zeros((num_classes, 1))\n",
    "    FP_all = np.zeros((num_classes, 1))\n",
    "    TN_all = np.zeros((num_classes, 1))\n",
    "    FN_all = np.zeros((num_classes, 1))\n",
    "    n_valid_sample_all = 0\n",
    "    F1 = np.zeros((num_classes, 1))\n",
    "    F1_best = 0.5  # Initial best F1 score\n",
    "\n",
    "    for _, batch in enumerate(test_loader):\n",
    "        image, label, _, name = batch\n",
    "        label = label.squeeze().numpy()\n",
    "        image = image.float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(image)\n",
    "\n",
    "        _, pred = torch.max(pred, 1)\n",
    "        pred = pred.squeeze().data.cpu().numpy()\n",
    "\n",
    "        TP, FP, TN, FN, n_valid_sample = eval_image(pred.reshape(-1), label.reshape(-1), num_classes)\n",
    "        TP_all += TP\n",
    "        FP_all += FP\n",
    "        TN_all += TN\n",
    "        FN_all += FN\n",
    "        n_valid_sample_all += n_valid_sample\n",
    "\n",
    "    # Calculate precision, recall, F1 score\n",
    "    OA = np.sum(TP_all) * 1.0 / n_valid_sample_all\n",
    "    for i in range(num_classes):\n",
    "        P = TP_all[i] * 1.0 / (TP_all[i] + FP_all[i] + 1e-14)\n",
    "        R = TP_all[i] * 1.0 / (TP_all[i] + FN_all[i] + 1e-14)\n",
    "        F1[i] = 2.0 * P * R / (P + R + 1e-14)\n",
    "\n",
    "        if i == 1:\n",
    "            print(f'===> Precision: {P*100:.2f} Recall: {R*100:.2f} F1: {F1[i]*100:.2f}')\n",
    "\n",
    "    mF1 = np.mean(F1)\n",
    "    print(f'===> Mean F1: {mF1*100:.2f} OA: {OA*100:.2f}')\n",
    "\n",
    "    # Save the best model based on F1 score\n",
    "    if F1[1] > F1_best:\n",
    "        F1_best = F1[1]\n",
    "        print('Saving Model...')\n",
    "        model_name = f'best_model_F1_{int(F1[1]*10000)}.pth'\n",
    "        torch.save(model.state_dict(), os.path.join(snapshot_dir, model_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10/500 Time: 16.29 Batch_OA = 76.8 cross_entropy_loss = 0.593\n",
      "Iter 20/500 Time: 20.39 Batch_OA = 97.8 cross_entropy_loss = 0.337\n",
      "Iter 30/500 Time: 12.57 Batch_OA = 97.9 cross_entropy_loss = 0.227\n",
      "Iter 40/500 Time: 13.58 Batch_OA = 98.4 cross_entropy_loss = 0.166\n",
      "Iter 50/500 Time: 12.51 Batch_OA = 98.1 cross_entropy_loss = 0.141\n",
      "Iter 60/500 Time: 12.52 Batch_OA = 98.5 cross_entropy_loss = 0.114\n",
      "Iter 70/500 Time: 12.49 Batch_OA = 98.3 cross_entropy_loss = 0.100\n",
      "Iter 80/500 Time: 12.46 Batch_OA = 98.5 cross_entropy_loss = 0.086\n",
      "Iter 90/500 Time: 12.47 Batch_OA = 98.5 cross_entropy_loss = 0.076\n",
      "Iter 100/500 Time: 13.03 Batch_OA = 98.7 cross_entropy_loss = 0.065\n",
      "Iter 110/500 Time: 12.46 Batch_OA = 98.6 cross_entropy_loss = 0.064\n",
      "Iter 120/500 Time: 12.36 Batch_OA = 98.3 cross_entropy_loss = 0.066\n",
      "Iter 130/500 Time: 12.62 Batch_OA = 98.5 cross_entropy_loss = 0.059\n",
      "Iter 140/500 Time: 13.22 Batch_OA = 98.5 cross_entropy_loss = 0.054\n",
      "Iter 150/500 Time: 15.69 Batch_OA = 98.8 cross_entropy_loss = 0.047\n",
      "Iter 160/500 Time: 15.25 Batch_OA = 98.6 cross_entropy_loss = 0.049\n",
      "Iter 170/500 Time: 15.74 Batch_OA = 98.6 cross_entropy_loss = 0.048\n",
      "Iter 180/500 Time: 12.60 Batch_OA = 98.6 cross_entropy_loss = 0.045\n",
      "Iter 190/500 Time: 14.65 Batch_OA = 98.5 cross_entropy_loss = 0.049\n",
      "Iter 200/500 Time: 14.39 Batch_OA = 98.5 cross_entropy_loss = 0.046\n",
      "Iter 210/500 Time: 12.59 Batch_OA = 98.6 cross_entropy_loss = 0.044\n",
      "Iter 220/500 Time: 12.06 Batch_OA = 98.7 cross_entropy_loss = 0.042\n",
      "Iter 230/500 Time: 12.31 Batch_OA = 98.6 cross_entropy_loss = 0.044\n",
      "Iter 240/500 Time: 13.70 Batch_OA = 98.6 cross_entropy_loss = 0.042\n",
      "Iter 250/500 Time: 12.10 Batch_OA = 98.6 cross_entropy_loss = 0.044\n",
      "Iter 260/500 Time: 12.03 Batch_OA = 98.6 cross_entropy_loss = 0.042\n",
      "Iter 270/500 Time: 11.98 Batch_OA = 98.7 cross_entropy_loss = 0.042\n",
      "Iter 280/500 Time: 12.04 Batch_OA = 98.6 cross_entropy_loss = 0.042\n",
      "Iter 290/500 Time: 12.01 Batch_OA = 98.4 cross_entropy_loss = 0.046\n",
      "Iter 300/500 Time: 12.01 Batch_OA = 98.7 cross_entropy_loss = 0.039\n",
      "Iter 310/500 Time: 12.25 Batch_OA = 98.7 cross_entropy_loss = 0.038\n",
      "Iter 320/500 Time: 12.06 Batch_OA = 98.5 cross_entropy_loss = 0.042\n",
      "Iter 330/500 Time: 12.12 Batch_OA = 98.5 cross_entropy_loss = 0.040\n",
      "Iter 340/500 Time: 12.84 Batch_OA = 98.5 cross_entropy_loss = 0.040\n",
      "Iter 350/500 Time: 12.46 Batch_OA = 98.8 cross_entropy_loss = 0.035\n",
      "Iter 360/500 Time: 12.53 Batch_OA = 98.7 cross_entropy_loss = 0.035\n",
      "Iter 370/500 Time: 12.38 Batch_OA = 98.6 cross_entropy_loss = 0.038\n",
      "Iter 380/500 Time: 12.40 Batch_OA = 98.7 cross_entropy_loss = 0.037\n",
      "Iter 390/500 Time: 12.32 Batch_OA = 98.6 cross_entropy_loss = 0.037\n",
      "Iter 400/500 Time: 12.33 Batch_OA = 98.7 cross_entropy_loss = 0.036\n",
      "Iter 410/500 Time: 12.35 Batch_OA = 98.7 cross_entropy_loss = 0.038\n",
      "Iter 420/500 Time: 12.39 Batch_OA = 98.3 cross_entropy_loss = 0.046\n",
      "Iter 430/500 Time: 12.45 Batch_OA = 98.5 cross_entropy_loss = 0.040\n",
      "Iter 440/500 Time: 12.48 Batch_OA = 98.6 cross_entropy_loss = 0.038\n",
      "Iter 450/500 Time: 12.46 Batch_OA = 98.6 cross_entropy_loss = 0.038\n",
      "Iter 460/500 Time: 12.29 Batch_OA = 98.7 cross_entropy_loss = 0.036\n",
      "Iter 470/500 Time: 12.14 Batch_OA = 98.4 cross_entropy_loss = 0.042\n",
      "Iter 480/500 Time: 12.00 Batch_OA = 98.7 cross_entropy_loss = 0.036\n",
      "Iter 490/500 Time: 11.96 Batch_OA = 98.4 cross_entropy_loss = 0.041\n",
      "Iter 500/500 Time: 11.90 Batch_OA = 98.5 cross_entropy_loss = 0.040\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. Run training and testing\n",
    "train_model(num_steps_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./snapshots/unet_trained_model.pth\n"
     ]
    }
   ],
   "source": [
    "# 7. Saving the Trained Model\n",
    "def save_model(model, snapshot_dir, model_name='unet_trained_model.pth'):\n",
    "    # Ensure the snapshot directory exists\n",
    "    if not os.path.exists(snapshot_dir):\n",
    "        os.makedirs(snapshot_dir)\n",
    "    \n",
    "    # Save the model's state_dict (recommended way of saving models)\n",
    "    model_path = os.path.join(snapshot_dir, model_name)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f'Model saved to {model_path}')\n",
    "\n",
    "# Example usage after your training loop:\n",
    "save_model(model, snapshot_dir, 'unet_trained_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing..........\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\cvlns\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\cvlns\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"d:\\VIT\\Project - 1\\Landslide detection\\Mine\\dataset\\landslide_dataset.py\", line 78, in __getitem__\n    with h5py.File(datafiles[\"label\"], \"r\") as hf:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\cvlns\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\h5py\\_hl\\files.py\", line 561, in __init__\n    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\cvlns\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\h5py\\_hl\\files.py\", line 235, in make_fid\n    fid = h5f.open(name, flags, fapl=fapl)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"h5py\\\\_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py\\\\_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"h5py\\\\h5f.pyx\", line 102, in h5py.h5f.open\nFileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = './dataset/TestData/mask/mask_1.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m, in \u001b[0;36mtest_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m F1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((num_classes, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     11\u001b[0m F1_best \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# Initial best F1 score\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:1465\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1464\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1491\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_utils.py:715\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 715\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\cvlns\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\cvlns\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"d:\\VIT\\Project - 1\\Landslide detection\\Mine\\dataset\\landslide_dataset.py\", line 78, in __getitem__\n    with h5py.File(datafiles[\"label\"], \"r\") as hf:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\cvlns\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\h5py\\_hl\\files.py\", line 561, in __init__\n    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\cvlns\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\h5py\\_hl\\files.py\", line 235, in make_fid\n    fid = h5f.open(name, flags, fapl=fapl)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"h5py\\\\_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py\\\\_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"h5py\\\\h5f.pyx\", line 102, in h5py.h5f.open\nFileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = './dataset/TestData/mask/mask_1.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "test_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
