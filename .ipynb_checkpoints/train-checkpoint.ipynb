{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train.ipynb\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 1. Importing required libraries\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mNetworks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unet\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# train.ipynb\n",
    "\n",
    "# 1. Importing required libraries\n",
    "import os\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from model.Networks import unet\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from dataset.landslide_dataset import LandslideDataSet\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 2. Defining the necessary configurations and settings\n",
    "# You can use argparse or hard-code these values based on your needs\n",
    "data_dir = './data/'  # Directory where your dataset is located\n",
    "train_list = './data/train.txt'  # Path to the train.txt file\n",
    "test_list = './data/test.txt'  # Path to the test.txt file\n",
    "input_size = '128,128'  # Input size for the images\n",
    "num_classes = 2  # Number of classes (Landslide, Non-Landslide)\n",
    "batch_size = 32  # Batch size for training\n",
    "num_workers = 4  # Number of workers for data loading\n",
    "learning_rate = 1e-3  # Learning rate\n",
    "num_steps = 5000  # Number of training steps\n",
    "num_steps_stop = 5000  # Number of training steps for early stopping\n",
    "weight_decay = 5e-4  # Weight decay for regularization\n",
    "snapshot_dir = './snapshots/'  # Directory to save model snapshots\n",
    "\n",
    "# Ensure that the snapshot directory exists\n",
    "if not os.path.exists(snapshot_dir):\n",
    "    os.makedirs(snapshot_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Loading the data\n",
    "def load_data():\n",
    "    src_loader = data.DataLoader(\n",
    "        LandslideDataSet(data_dir, train_list, max_iters=num_steps_stop * batch_size, set='labeled'),\n",
    "        batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    test_loader = data.DataLoader(\n",
    "        LandslideDataSet(data_dir, test_list, set='labeled'),\n",
    "        batch_size=1, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return src_loader, test_loader\n",
    "\n",
    "src_loader, test_loader = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Initializing the U-Net model\n",
    "model = unet(n_classes=num_classes)\n",
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Defining the optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "cross_entropy_loss = nn.CrossEntropyLoss(ignore_index=255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Training loop\n",
    "hist = np.zeros((num_steps_stop, 3))\n",
    "F1_best = 0.5\n",
    "\n",
    "for batch_id, src_data in enumerate(src_loader):\n",
    "    if batch_id == num_steps_stop:\n",
    "        break\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    images, labels, _, _ = src_data\n",
    "    images = images.cuda()\n",
    "    \n",
    "    # Forward pass\n",
    "    pred = model(images)\n",
    "    \n",
    "    # Compute loss\n",
    "    labels = labels.cuda().long()\n",
    "    loss = cross_entropy_loss(pred, labels)\n",
    "    \n",
    "    # Backpropagation and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate batch accuracy\n",
    "    _, predicted_labels = torch.max(pred, 1)\n",
    "    predicted_labels = predicted_labels.detach().cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    \n",
    "    batch_oa = np.sum(predicted_labels == labels) * 1.0 / len(labels.reshape(-1))\n",
    "    hist[batch_id, 0] = loss.item()\n",
    "    hist[batch_id, 1] = batch_oa\n",
    "    hist[batch_id, -1] = time.time() - start_time\n",
    "\n",
    "    # Print progress\n",
    "    if (batch_id + 1) % 10 == 0:\n",
    "        print(f'Iter {batch_id+1}/{num_steps} Time: {np.mean(hist[batch_id-9:batch_id+1,-1]):.2f} Batch_OA = {np.mean(hist[batch_id-9:batch_id+1,1])*100:.1f} cross_entropy_loss = {np.mean(hist[batch_id-9:batch_id+1,0]):.3f}')\n",
    "    \n",
    "    # Evaluation every 500 iterations\n",
    "    if (batch_id + 1) % 500 == 0:\n",
    "        print('Testing..........')\n",
    "        model.eval()\n",
    "        \n",
    "        TP_all = np.zeros((num_classes, 1))\n",
    "        FP_all = np.zeros((num_classes, 1))\n",
    "        TN_all = np.zeros((num_classes, 1))\n",
    "        FN_all = np.zeros((num_classes, 1))\n",
    "        n_valid_sample_all = 0\n",
    "        F1 = np.zeros((num_classes, 1))\n",
    "        \n",
    "        for _, batch in enumerate(test_loader):\n",
    "            image, label, _, name = batch\n",
    "            label = label.squeeze().numpy()\n",
    "            image = image.float().cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = model(image)\n",
    "\n",
    "            _, pred = torch.max(pred, 1)\n",
    "            pred = pred.squeeze().data.cpu().numpy()\n",
    "\n",
    "            TP, FP, TN, FN, n_valid_sample = eval_image(pred.reshape(-1), label.reshape(-1), num_classes)\n",
    "            TP_all += TP\n",
    "            FP_all += FP\n",
    "            TN_all += TN\n",
    "            FN_all += FN\n",
    "            n_valid_sample_all += n_valid_sample\n",
    "\n",
    "        # Calculate precision, recall, F1 score\n",
    "        OA = np.sum(TP_all) * 1.0 / n_valid_sample_all\n",
    "        for i in range(num_classes):\n",
    "            P = TP_all[i] * 1.0 / (TP_all[i] + FP_all[i] + 1e-14)\n",
    "            R = TP_all[i] * 1.0 / (TP_all[i] + FN_all[i] + 1e-14)\n",
    "            F1[i] = 2.0 * P * R / (P + R + 1e-14)\n",
    "            \n",
    "            if i == 1:\n",
    "                print(f'===> {name_classes[i]} Precision: {P*100:.2f}')\n",
    "                print(f'===> {name_classes[i]} Recall: {R*100:.2f}')\n",
    "                print(f'===> {name_classes[i]} F1: {F1[i]*100:.2f}')\n",
    "        \n",
    "        mF1 = np.mean(F1)\n",
    "        print(f'===> Mean F1: {mF1*100:.2f} OA: {OA*100:.2f}')\n",
    "\n",
    "        # Save the best model based on F1 score\n",
    "        if F1[1] > F1_best:\n",
    "            F1_best = F1[1]\n",
    "            print('Saving Model...')\n",
    "            model_name = f'batch{batch_id+1}_F1_{int(F1[1]*10000)}.pth'\n",
    "            torch.save(model.state_dict(), os.path.join(snapshot_dir, model_name))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
